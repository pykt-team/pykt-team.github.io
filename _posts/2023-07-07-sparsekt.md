---
layout: post
title: 'Towards Robust Knowledge Tracing Models via k-Sparse Attention'
date: 2022-07-13T16:00:00.000+00:00
tags: model
categories: []
author: ''
post_image: "/assets/images/posts/sparsekt.png"
post_format: ''
trending: true

---
We added sparseKT into our pyKT package.

The link is [here](https://pykt-toolkit.readthedocs.io/en/latest/models.html#sparsekt) and the API is [here](https://pykt-toolkit.readthedocs.io/en/latest/pykt.models.html#module-pykt.models.sparsekt).

Original paper can be found at [Huang, Shuyan, et al. "Towards Robust Knowledge Tracing Models via k-Sparse Attention."]

Title: simpleKT: A Simple But Tough-to-Beat Baseline for Knowledge Tracing

Author:Shuyan Huang, [Zitao Liu](https://scholar.google.com/citations?user=rRTzNm0AAAAJ&hl=en&oi=sra), [Xiangyu Zhao](https://scholar.google.com/citations?user=Nkm9r2IAAAAJ&hl=en&oi=ao), Weiqi Luo, [Jian Weng](https://scholar.google.com/citations?user=7xRkSZAAAAAJ&hl=en&oi=ao)

Knowledge tracing (KT) is the problem of predicting students' future performance based on their historical interaction sequences. With the advanced capability of capturing contextual long-term dependency, attention mechanism becomes one of the essential components in many deep learning based KT (DLKT) models. In spite of the impressive performance achieved by these attentional DLKT models, many of them are often vulnerable to run the risk of overfitting, especially on small-scale educational datasets. Therefore, in this paper, we propose \textsc{sparseKT}, a simple yet effective framework to facilitate the robustness and generalization of the attention based DLKT approaches. To learn the sparse attention from KT data we incorporate a k-selection module into standard attention function to only pick items with highest attention scores. We propose two sparsification heuristics : (1) soft-thresholding sparse attention and (2) top-K sparse attention. We show that our \textsc{sparseKT} is able to help attentional KT models get rid of irrelevant or useless student interactions and improves the predictive performance when compared to 12 state-of-the-art on three publicly available real-world educational datasets. To encourage reproducible research, we make our data and code publicly available at \url{https://tinyurl.com/366fwdjn}.

