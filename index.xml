<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://pykt.org/index.xml" rel="self" type="application/atom+xml" /><link href="https://pykt.org/" rel="alternate" type="text/html" /><updated>2025-04-26T16:43:03+08:00</updated><id>https://pykt.org/index.xml</id><entry><title type="html">Enhancing Knowledge Tracing through Decoupling Cognitive Pattern from Error-Prone Data</title><link href="https://pykt.org/roubstkt" rel="alternate" type="text/html" title="Enhancing Knowledge Tracing through Decoupling Cognitive Pattern from Error-Prone Data" /><published>2025-04-26T00:00:00+08:00</published><updated>2025-04-26T00:00:00+08:00</updated><id>https://pykt.org/roubstkt</id><content type="html" xml:base="https://pykt.org/roubstkt"><![CDATA[<p>We added RoubstKT into our pyKT package.</p>

<p>The link is <a href="https://pykt-toolkit.readthedocs.io/en/latest/models.html#roubstkt">here</a> and the API is <a href="https://pykt-toolkit.readthedocs.io/en/latest/pykt.models.html#module-pykt.models.roubstkt">here</a>.</p>

<p>Original paper can be found at Guo,Teng et al. “Enhancing Knowledge Tracing through Decoupling Cognitive Pattern from Error-Prone Data.” <a href="https://dl.acm.org/doi/10.1145/3696410.3714486">The ACM on Web Conference. 2025.
</a></p>

<p>Title: Enhancing Knowledge Tracing through Decoupling Cognitive Pattern from Error-Prone Data</p>

<p>Abstract:  Knowledge tracing aims to predict students’ future performance based on their past learning activities. However, no one is perfect. Factors such as carelessness, fatigue, and stress often cause students to make mistakes on problems they have already mastered, leading to anomalies in their historical learning data. These anomalies disrupt inherent patterns in the data, misleading the KT model. Extracting cognitive patterns that accurately reflect students’ knowledge mastery from such error-prone data remains a significant challenge. Against this background, this paper proposes a new knowledge tracing method named RoubstKT, inspired by educational measurement theory and frequency-based decomposition. A cognitive decoupling analyzer is proposed to decouple the student’s cognitive pattern and random factors from the data through smoothing and subtraction operations, then recombine them using a gating mechanism or adaptive parameter fusion strategy. To more effectively diagnose students’ knowledge mastery, we employ a decay-based attention mechanism that focuses on random behaviors at adjacent time steps. We conducted comprehensive experiments based on real-world datasets and targeted datasets with added random noise. The experimental results demonstrated the effectiveness of the proposed method.</p>]]></content><author><name></name></author><category term="model" /><summary type="html"><![CDATA[We added RoubstKT into our pyKT package.]]></summary></entry><entry><title type="html">HCGKT: Hierarchical Contrastive Graph Knowledge Tracing with Multi-level Feature Learning</title><link href="https://pykt.org/hcgkt" rel="alternate" type="text/html" title="HCGKT: Hierarchical Contrastive Graph Knowledge Tracing with Multi-level Feature Learning" /><published>2025-04-16T00:00:00+08:00</published><updated>2025-04-16T00:00:00+08:00</updated><id>https://pykt.org/hcgkt</id><content type="html" xml:base="https://pykt.org/hcgkt"><![CDATA[<p>We added HCGKT into our pyKT package.</p>

<p>The link is <a href="https://pykt-toolkit.readthedocs.io/en/latest/models.html#hcgkt">here</a> and the API is <a href="https://pykt-toolkit.readthedocs.io/en/latest/pykt.models.html#module-pykt.models.hcgkt">here</a>.</p>

<p>Original paper can be found at ZhiW,Huang et al. “HCGKT: Hierarchical Contrastive Graph Knowledge Tracing with Multi-level Feature Learning.” <a href="">International Conference on Artificial Intelligence in Education. 2025.
</a></p>

<p>Title: HCGKT: Hierarchical Contrastive Graph Knowledge Tracing with Multi-level Feature Learning</p>

<p>Abstract: Knowledge Tracing (KT) aims to predict learners’ future performance by modeling their historical interaction data. Despite recent advances in attention-based KT models, significant challenges remain: effectively capturing hierarchical relationships between questions and knowledge components (KCs), handling noisy educational data, and accurately modeling complex semantic relationships. To address these challenges, we propose Hierarchical Contrastive Graph Knowledge Tracing (HCGKT), which combines hierarchical graph filtering attention, adversarial contrastive learning, and graph convolutional networks. Experiments on three datasets demonstrate our model’s superior performance in both prediction accuracy and interpretability. We have provided all the datasets and code at https://pykt.org/.</p>]]></content><author><name></name></author><category term="model" /><summary type="html"><![CDATA[We added HCGKT into our pyKT package.]]></summary></entry><entry><title type="html">Uncertainty-aware Knowledge Tracing</title><link href="https://pykt.org/ukt" rel="alternate" type="text/html" title="Uncertainty-aware Knowledge Tracing" /><published>2025-03-17T00:00:00+08:00</published><updated>2025-03-17T00:00:00+08:00</updated><id>https://pykt.org/ukt</id><content type="html" xml:base="https://pykt.org/ukt"><![CDATA[<p>We added UKT into our pyKT package.</p>

<p>The link is <a href="https://pykt-toolkit.readthedocs.io/en/latest/models.html#ukt">here</a> and the API is <a href="https://pykt-toolkit.readthedocs.io/en/latest/pykt.models.html#module-pykt.models.ukt">here</a>.</p>

<p>Original paper can be found at Cheng W, Du H, Li C, et al. “Uncertainty-aware Knowledge Tracing.” <a href="https://arxiv.org/abs/2501.05415">Proceedings of the 39th Annual AAAI Conference on Artificial Intelligence. 2025.
</a></p>

<p>Title: Uncertainty-aware Knowledge Tracing</p>

<p>Abstract: Knowledge Tracing (KT) is crucial in education assessment,
which focuses on depicting students’ learning states and assessing students’ mastery of subjects. With the rise of modern online learning platforms, particularly massive open online courses (MOOCs), an abundance of interaction data has greatly advanced the development of the KT technology. Previous research commonly adopts deterministic representation to capture students’ knowledge states, which neglects the uncertainty during student interactions and thus fails to model
the true knowledge state in learning process. In light of this, we propose an Uncertainty-Aware Knowledge Tracing model
(UKT) which employs stochastic distribution embeddings to represent the uncertainty in student interactions, with a
Wasserstein self-attention mechanism designed to capture the transition of state distribution in student learning behaviors.
Additionally, we introduce the aleatory uncertainty-aware contrastive learning loss, which strengthens the model’s robustness towards different types of uncertainties. Extensive experiments on six real-world datasets demonstrate that UKT
not only significantly surpasses existing deep learning-based models in KT prediction, but also shows unique advantages
in handling the uncertainty of student interactions.</p>]]></content><author><name></name></author><category term="model" /><summary type="html"><![CDATA[We added UKT into our pyKT package.]]></summary></entry><entry><title type="html">Cognitive Fluctuations Enhanced Attention Network for Knowledge Tracing</title><link href="https://pykt.org/fluckt" rel="alternate" type="text/html" title="Cognitive Fluctuations Enhanced Attention Network for Knowledge Tracing" /><published>2025-02-16T00:00:00+08:00</published><updated>2025-02-16T00:00:00+08:00</updated><id>https://pykt.org/fluckt</id><content type="html" xml:base="https://pykt.org/fluckt"><![CDATA[<p>We added flucKT into our pyKT package.</p>

<p>The link is <a href="https://pykt-toolkit.readthedocs.io/en/latest/models.html#fluckt">here</a> and the API is <a href="https://pykt-toolkit.readthedocs.io/en/latest/pykt.models.html#module-pykt.models.fluckt">here</a>.</p>

<p>Original paper can be found at <a href="https://drive.google.com/file/d/1tRW2j5cmjj5asYMwNvd2z6BEB313QcSg/view">Hou M, Li X, Guo T, et al. “Cognitive Fluctuations Enhanced Attention Network for Knowledge Tracing.” Proceedings of the 39th Annual AAAI Conference on Artificial Intelligence. 2025.
</a></p>

<p>Title: Cognitive Fluctuations Enhanced Attention Network for Knowledge Tracing</p>

<p>Abstract: Knowledge tracing (KT) involves using the historical recordsof student-learning interactions to anticipate their performance on forthcoming questions. Central to this process is the modeling of human cognition to gain deeper insights into how knowledge is acquired and retained. Human cognition is characterized by two key features: long-term cognitive trends, reflecting the gradual accumulation and stabilization of knowledge over time, and short-term cognitive fluctuations, which arise from transient factors such as forgetting or momentary lapses in attention. Although existing attention-based KT models effectively capture long-term cognitive trends, they often fail to adequately address short-term cognitive fluctuations. These limitations lead to overly smoothed cognitive features and reduced model performance, especially when the test data length exceeds the training data length. To address these problems, we propose FlucKT, a novel shortterm cognitive fluctuations enhanced attention network for KT tasks. FlucKT improves the attention mechanism in two ways: First, by using a decomposition-based layer with causal convolution to separate and dynamically reweight long-term and short-term cognitive features. Second, by introducing a kernelized bias attention score penalty to enhance focus on short-term fluctuations, improving length generalization capabilities. Our contributions are validated through extensive experiments on three real-world datasets, demonstrating significant improvements in length generalization and prediction performance.</p>]]></content><author><name></name></author><category term="model" /><summary type="html"><![CDATA[We added flucKT into our pyKT package.]]></summary></entry><entry><title type="html">Rethinking and Improving Student Learning and Forgetting Processes for Attention Based Knowledge Tracing Models</title><link href="https://pykt.org/lefokt" rel="alternate" type="text/html" title="Rethinking and Improving Student Learning and Forgetting Processes for Attention Based Knowledge Tracing Models" /><published>2025-02-16T00:00:00+08:00</published><updated>2025-02-16T00:00:00+08:00</updated><id>https://pykt.org/lefokt</id><content type="html" xml:base="https://pykt.org/lefokt"><![CDATA[<p>We added LefoKT into our pyKT package.</p>

<p>The link is <a href="https://pykt-toolkit.readthedocs.io/en/latest/models.html#lefokt">here</a> and the API is <a href="https://pykt-toolkit.readthedocs.io/en/latest/pykt.models.html#module-pykt.models.lefokt">here</a>.</p>

<p>Original paper can be found at <a href="https://aaai.org/conference/aaai/aaai-25/">Bai Y, Li X, Liu Z, et al. “Rethinking and Improving Student Learning and Forgetting Processes for Attention Based Knowledge Tracing Models.” Proceedings of the 39th Annual AAAI Conference on Artificial Intelligence. 2025.
</a></p>

<p>Title: Rethinking and Improving Student Learning and Forgetting Processes for Attention Based Knowledge Tracing Models</p>

<p>Abstract: Knowledge tracing (KT) models students’ knowledge states and predicts their future performance based on their historical interaction data. However, attention based KT models
struggle to accurately capture diverse forgetting behaviors in ever-growing interaction sequences. First, existing models use uniform time decay matrices, conflating forgetting representations with problem relevance. Second, the fixed-length window prediction paradigm fails to model continuous forgetting processes in expanding sequences. To address these
challenges, this paper introduces LefoKT, a unified architecture that enhances attention based KT models by incorporating proposed relative forgetting attention. LefoKT improves
forgetting modeling through relative forgetting attention to decouple forgetting patterns from problem relevance. It also enhances attention based KT models’ length extrapolation capability for capturing continuous forgetting processes in ever-growing interaction sequences. Extensive experimental results on three datasets validate the effectiveness of LefoKT.</p>]]></content><author><name></name></author><category term="model" /><summary type="html"><![CDATA[We added LefoKT into our pyKT package.]]></summary></entry><entry><title type="html">csKT: Addressing Cold-start Problem in Knowledge Tracing via Kernel Bias and Cone Attention</title><link href="https://pykt.org/cskt" rel="alternate" type="text/html" title="csKT: Addressing Cold-start Problem in Knowledge Tracing via Kernel Bias and Cone Attention" /><published>2025-01-13T00:00:00+08:00</published><updated>2025-01-13T00:00:00+08:00</updated><id>https://pykt.org/cskt</id><content type="html" xml:base="https://pykt.org/cskt"><![CDATA[<p>We added csKT into our pyKT package.</p>

<p>The link is <a href="https://pykt-toolkit.readthedocs.io/en/latest/models.html#cskt">here</a> and the API is <a href="https://pykt-toolkit.readthedocs.io/en/latest/pykt.models.html#module-pykt.models.cskt">here</a>.</p>

<p>Original paper can be found at <a href="https://www.sciencedirect.com/science/article/pii/S0957417424028550">Bai Y, Li X, Liu Z, et al. “csKT: Addressing Cold-start Problem in Knowledge Tracing via Kernel Bias and Cone Attention.” Proceedings of the Expert Systems with Applications. 2025.
</a></p>

<p>Title: csKT: Addressing Cold-start Problem in Knowledge Tracing via Kernel Bias and Cone Attention</p>

<p>Abstract: Knowledge tracing (KT) is the task of predicting students’ future performances based on their past interactions in online learning systems. When new students enter the system with short interaction sequences, the cold-start problem commonly arises in KT. Although existing deep learning based KT models exhibit impressive performance, it remains challenging for these models to be trained on short student interaction sequences and maintain stable prediction accuracy as the number of student interactions increases. In this paper, we propose cold-start KT (csKT) to address this problem. Specifically, csKT employs kernel bias to guide learning from short sequences and ensure accurate predictions for longer sequences, and it also introduces cone attention to better capture complex hierarchical relationships between knowledge components in cold-start scenarios. We evaluate csKT on four public real-world educational datasets, where it demonstrated superior performance over a broad range of deep learning based KT models using common evaluation metrics in cold-start scenarios. Additionally, we conduct ablation studies and produce visualizations to verify the effectiveness of our csKT model.</p>]]></content><author><name></name></author><category term="model" /><summary type="html"><![CDATA[We added csKT into our pyKT package.]]></summary></entry><entry><title type="html">Revisiting Knowledge Tracing: A Simple and Powerful Model</title><link href="https://pykt.org/rekt" rel="alternate" type="text/html" title="Revisiting Knowledge Tracing: A Simple and Powerful Model" /><published>2024-11-19T00:00:00+08:00</published><updated>2024-11-19T00:00:00+08:00</updated><id>https://pykt.org/rekt</id><content type="html" xml:base="https://pykt.org/rekt"><![CDATA[<p>We added reKT into our pyKT package.</p>

<p>The link is <a href="https://pykt-toolkit.readthedocs.io/en/latest/models.html#rekt">here</a> and the API is <a href="https://pykt-toolkit.readthedocs.io/en/latest/pykt.models.html#module-pykt.models.rekt">here</a>.</p>

<p>Original paper can be found at <a href="https://dl.acm.org/doi/pdf/10.1145/3664647.3681205">Shen, Xiaoxuan, et al. “Revisiting Knowledge Tracing: A Simple and Powerful Model.” Proceedings of the 32nd ACM International Conference on Multimedia. 2024.”.
</a></p>

<p>Title: Revisiting Knowledge Tracing: A Simple and Powerful Model</p>

<p>Abstract: Advances in multimedia technology and its widespread application in education have made multimedia learning increasingly important. Knowledge Tracing (KT) is the key technology for achieving adaptive multimedia learning, aiming to monitor the degree of knowledge acquisition and predict students’ performance during the learning process. Current KT research is dedicated to enhancing the performance of KT problems by integrating the most advanced deep learning techniques. However, this has led to increasingly complex models, which reduce model usability and divert researchers’ attention away from exploring the core issues of KT. This paper aims to tackle the fundamental challenges of KT tasks, including the knowledge state representation and the core architecture design, and investigate a novel KT model that is both simple and powerful. We have revisited the KT task and propose the ReKT model. First, taking inspiration from the decision-making process of human teachers, we model the knowledge state of students from three distinct perspectives: questions, concepts, and domains. Second, building upon human cognitive development models, such as constructivism, we have designed a Forget-Response-Update (FRU) framework to serve as the core architecture for the KT task. The FRU is composed of just two linear regression units, making it an extremely lightweight framework. Extensive comparisons were conducted with 22 state-of-the-art KT models on 7 publicly available datasets. The experimental results demonstrate that ReKT outperforms all the comparative methods in question-based KT tasks, and consistently achieves the best (in most cases) or near-best performance in concept-based KT tasks. Furthermore, in comparison to other KT core architectures like Transformers or LSTMs, the FRU achieves superior prediction performance with approximately only 38% computing resources. Through an exploration of the ReKT model that is both simple and powerful, is able to offer new insights to future KT research. The code can be found at https://github.com/lilstrawberry/ReKT.</p>]]></content><author><name></name></author><category term="model" /><summary type="html"><![CDATA[We added reKT into our pyKT package.]]></summary></entry><entry><title type="html">extraKT: Extending Context Window of Attention Based Knowledge Tracing Models via Length Extrapolation</title><link href="https://pykt.org/extrakt" rel="alternate" type="text/html" title="extraKT: Extending Context Window of Attention Based Knowledge Tracing Models via Length Extrapolation" /><published>2024-09-19T00:00:00+08:00</published><updated>2024-09-19T00:00:00+08:00</updated><id>https://pykt.org/extrakt</id><content type="html" xml:base="https://pykt.org/extrakt"><![CDATA[<p>We added extraKT into our pyKT package.</p>

<p>The link is <a href="https://pykt-toolkit.readthedocs.io/en/latest/models.html#extrakt">here</a> and the API is <a href="https://pykt-toolkit.readthedocs.io/en/latest/pykt.models.html#module-pykt.models.extrakt">here</a>.</p>

<p>Original paper can be found at <a href="[https://www.ijcai.org/proceedings/2024/654](https://www.ecai2024.eu/programme/accepted-papers#main-track)">Li X, Bai Y, Guo T, et al. “Extending Context Window of Attention Based Knowledge Tracing Models via Length Extrapolation.” Proceedings of the 26th European Conference on Artificial Intelligence. 2024.</a></p>

<p>Title: extraKT: Extending Context Window of Attention Based Knowledge Tracing Models via Length Extrapolation</p>

<p>Abstract: Knowledge tracing (KT) is a prediction task that aims to predict students’ future performance based on their past learning data. The rapid progress in attention mechanisms has led to the emergence of various high-performing attention based KT models. However, in online or personalized education settings, students’ varying learning paths result in different lengths of student interaction sequences, which poses a significant challenge for attention based KT models as their context window sizes are fixed during both training and prediction stages. We refer to this as the length extrapolation of KT model. In this paper, we propose extraKT to facilitate better extrapolation that learn from student interactions with a short context window and continue to perform well across various longer context window sizes at prediction stage. Specifically, we negatively bias attention scores with linearly decreasing penalties that are proportional to query-key distance, which efficiently represents short-term forgetting characteristics of student knowledge states. We conduct comprehensive and rigorous experiments on three real-world educational datasets. The results show that our extraKT model exhibits robust length extrapolation capability and outperforms state-of-the-art baseline models in terms of AUC and accuracy.</p>]]></content><author><name></name></author><category term="model" /><summary type="html"><![CDATA[We added extraKT into our pyKT package.]]></summary></entry><entry><title type="html">stableKT: Enhancing Length Generalization for Attention Based Knowledge Tracing Models with Linear Biases</title><link href="https://pykt.org/stablekt" rel="alternate" type="text/html" title="stableKT: Enhancing Length Generalization for Attention Based Knowledge Tracing Models with Linear Biases" /><published>2024-09-19T00:00:00+08:00</published><updated>2024-09-19T00:00:00+08:00</updated><id>https://pykt.org/stablekt</id><content type="html" xml:base="https://pykt.org/stablekt"><![CDATA[<p>We added stableKT into our pyKT package.</p>

<p>The link is <a href="https://pykt-toolkit.readthedocs.io/en/latest/models.html#stablekt">here</a> and the API is <a href="https://pykt-toolkit.readthedocs.io/en/latest/pykt.models.html#module-pykt.models.stablekt">here</a>.</p>

<p>Original paper can be found at <a href="https://www.ijcai.org/proceedings/2024/654">Li X, Bai Y, Guo T, et al. “Enhancing Length Generalization for Attention Based Knowledge Tracing Models with Linear Biases.” Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence. 2024.
</a></p>

<p>Title: stableKT: Enhancing Length Generalization for Attention Based Knowledge Tracing Models with Linear Biases</p>

<p>Abstract: Knowledge tracing (KT) is the task of predicting students’ future performance based on their historical learning interaction data. With the rapid advancement of attention mechanisms, many attention based KT models are developed. However, existing attention based KT models exhibit performance drops as the number of student interactions increases beyond the number of interactions on which the KT models are trained. We refer to this as the length generalization of KT model. In this paper, we propose stableKT to enhance length generalization that is able to learn from short sequences and maintain high prediction performance when generalizing on long sequences. Furthermore, we design a multi-head aggregation module to capture the complex relationships between questions and the corresponding knowledge components (KCs) by combining dot-product attention and hyperbolic attention. Experimental results on three public educational datasets show that our model exhibits robust capability of length generalization and outperforms all baseline models in terms of AUC.</p>]]></content><author><name></name></author><category term="model" /><summary type="html"><![CDATA[We added stableKT into our pyKT package.]]></summary></entry><entry><title type="html">FoLiBiKT: Forgetting-aware Linear Bias for Attentive Knowledge Tracing</title><link href="https://pykt.org/folibikt" rel="alternate" type="text/html" title="FoLiBiKT: Forgetting-aware Linear Bias for Attentive Knowledge Tracing" /><published>2024-03-09T00:00:00+08:00</published><updated>2024-03-09T00:00:00+08:00</updated><id>https://pykt.org/folibikt</id><content type="html" xml:base="https://pykt.org/folibikt"><![CDATA[<p>We added FoLiBiKT into our pyKT package.</p>

<p>The link is <a href="https://pykt-toolkit.readthedocs.io/en/latest/models.html#folibikt">here</a> and the API is <a href="https://pykt-toolkit.readthedocs.io/en/latest/pykt.models.html#module-pykt.models.folibikt">here</a>.</p>

<p>Original paper can be found at <a href="https://dl.acm.org/doi/abs/10.1145/3583780.3615191">Im, Yoonjin, et al. “Forgetting-aware Linear Bias for Attentive Knowledge Tracing.” Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. 2023.
</a></p>

<p>Title: FoLiBiKT: Forgetting-aware Linear Bias for Attentive Knowledge Tracing</p>

<p>Abstract: Knowledge Tracing (KT) aims to track proficiency based on a question-solving history, allowing us to offer a streamlined curriculum. Recent studies actively utilize attention-based mechanisms to capture the correlation between questions and combine it with the learner’s characteristics for responses. However, our empirical
study shows that existing attention-based KT models neglect the learner’s forgetting behavior, especially as the interaction history becomes longer. This problem arises from the bias that overprioritizes the correlation of questions while inadvertently ignoring the impact of forgetting behavior. This paper proposes a simple-yet-effective solution, namely Forgetting-aware Linear Bias (FoLiBi), to reflect forgetting behavior as a linear bias. Despite its simplicity, FoLiBi is readily equipped with existing attentive KT models by
effectively decomposing question correlations with forgetting behavior. FoLiBi plugged with several KT models yields a consistent improvement of up to 2.58% in AUC over state-of-the-art KT models on four benchmark datasets.</p>]]></content><author><name></name></author><category term="model" /><summary type="html"><![CDATA[We added FoLiBiKT into our pyKT package.]]></summary></entry></feed>